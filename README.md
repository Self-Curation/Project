# Preference Consistency Matters: Enhancing Preference Learning in Language Models with Automated Self-Curation of Training Corpora



----

We propose a simple yet effective Self-Curation method for training data that enhances preference learning by leveraging in-distribution trained proxy models.


<p align='center'>
<img src="./figure/self_curation_flow.png"  width="400" height="450" center-align="true">
<div align="center"><b>The proposed self-curation method </b></div>
</p>

<details>
<summary>Comparing Direct Preference Optimization (DPO) and its variants (cDPO and rDPO) with Self-Curation to no curation</summary>

<p align='center'>
<img src="./figure/improvement.png"  width="1000" height="220" center-align="true">
<div align="center"><b>DPO/cDPO/rDPO with Self-Curation significantly and consistently surpasses DPO/cDPO/rDPO without curation</b></div>
</p>
</details>

## Citation
@misc{<br>
lee2025preferenceconsistencymattersenhancing,<br>
      title={Preference Consistency Matters: Enhancing Preference Learning in Language Models with Automated Self-Curation of Training Corpora}, <br>
      author={JoonHo Lee and JuYoun Son and Juree Seok and Wooseok Jang and Yeong-Dae Kwon}, <br>
      year={2025}, <br>
      eprint={2408.12799}, <br>
      archivePrefix={arXiv}, <br>
      primaryClass={cs.CL}, <br>
      url={https://arxiv.org/abs/2408.12799},  <br>
}
